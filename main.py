# -*- coding: utf-8 -*-
"""alexnet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aQDw6yi0kUX4ZlObX7RXxIlTBX3o57js
"""

import torch
import torchvision
import torchvision.transforms as transforms
from torchvision import datasets
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

device = 'cuda' if torch.cuda.is_available() else 'cpu'

transform = transforms.Compose([
    # transforms.Resize(224),  # Resize to AlexNet input size
    transforms.ToTensor(),   # This converts to (3, 224, 224) format
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])

# Load CIFAR-10 dataset
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                     download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,
                                      shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                    download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=32,
                                     shuffle=False, num_workers=2)
Model = nn.Sequential(
    # Reduced number of filters (channels) in each conv layer
    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),  # 96->32
    nn.ReLU(),
    nn.MaxPool2d(2, 2),  # Added pooling to reduce dimensions

    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),  # 256->64
    nn.ReLU(),
    nn.MaxPool2d(2, 2),  # This will make it 8x8

    nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3, stride=1, padding=1),  # 384->96
    nn.ReLU(),
    nn.MaxPool2d(2, 2),  # This will make it 4x4

    nn.Flatten(),
    # Much smaller fully connected layers
    nn.Linear(96 * 4 * 4, 512),  # Much smaller than 4096
    nn.ReLU(),
    nn.Linear(512, 10),  # Direct to output
).to(device)

def train(model, epochs=10):
  loss_fn = nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum= 0.9, weight_decay=0.0005)
  train_losses = []
  test_losses = []

  for epoch in range(epochs):
    model.train()
    for (batch, targets) in trainloader:
      targets_onehot = F.one_hot(targets, num_classes=10).type(torch.float32)


      optimizer.zero_grad()
      preds = model(batch)
      loss = loss_fn(preds, targets_onehot)
      loss.backward()
      optimizer.step()

      print(f"TRAIN LOSS: {loss.item()}")
      train_losses.append(loss.item())

    model.eval()
    with torch.no_grad():
      for (batch, targets) in testloader:
        targets_onehot = F.one_hot(targets, num_classes=10).type(torch.float32)

        preds = model(batch)
        loss = loss_fn(preds, targets_onehot)

        print(f"TEST LOSS: {loss.item()}")
        test_losses.append(loss.item())

    return (train_losses, test_losses)

(train_losses, test_losses) = train(Model)

plt.figure(figsize=(10, 6))  # Creates a figure with specified size
plt.plot(train_losses)  # Plot the actual curve
plt.title('Training Loss Over Time')  # Add a title
plt.xlabel('Epoch')  # Label for x-axis
plt.ylabel('Loss')  # Label for y-axis
plt.grid(True)  # Add a grid for better readability
plt.show()  # Display the plot

plt.figure(figsize=(10, 6))  # Creates a figure with specified size
plt.plot(test_losses)  # Plot the actual curve
plt.title('Test Loss Over Time')  # Add a title
plt.xlabel('Epoch')  # Label for x-axis
plt.ylabel('Loss')  # Label for y-axis
plt.grid(True)  # Add a grid for better readability
plt.show()  # Display the plot

i = 54
print(testset.classes)
x = torch.tensor(testset.data)[i:i+1]
print(x.shape)
print(f"pred: {torch.argmax(Model(x.permute(0, 3, 1, 2).type(torch.float32)))}")
print(f"real: {testset.targets[i]}")
plt.imshow(x.squeeze())
